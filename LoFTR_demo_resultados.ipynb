{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vcYjx3HFUcw"
      },
      "source": [
        "# LoFTR demo resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CChK3ciimW4r"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/zju3dv/LoFTR.git\n",
        "%cd LoFTR\n",
        "#!pip install -r requirements.txt  # contiene torch, torchvision, etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 --force-reinstall"
      ],
      "metadata": {
        "id": "9_EoPB8gMwic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python==4.5.5.64\n",
        "!pip install albumentations==1.3.1\n",
        "!pip install ray einops==0.3.0 loguru yacs tqdm matplotlib h5py joblib\n",
        "!pip install kornia==0.6.7\n",
        "!pip install torchmetrics==0.11.4 pytorch-lightning==1.9.4"
      ],
      "metadata": {
        "id": "Ma8UsGR3IVLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, torch, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import entropy\n",
        "from loftr import LoFTR, default_cfg\n",
        "\n",
        "# Preparar modelo\n",
        "device = torch.device(\"cpu\")\n",
        "cfg = default_cfg\n",
        "matcher = LoFTR(cfg=cfg)\n",
        "matcher.to(device).eval()\n",
        "\n",
        "def extract_loftr(img1, img2):\n",
        "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)[None][None]\n",
        "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)[None][None]\n",
        "    t1 = torch.from_numpy(img1_gray / 255.).float().to(device)\n",
        "    t2 = torch.from_numpy(img2_gray / 255.).float().to(device)\n",
        "    with torch.no_grad():\n",
        "        input_dict = {\"image0\": t1, \"image1\": t2}\n",
        "        matched = matcher(input_dict)\n",
        "    mkpts0 = matched[\"keypoints0\"].cpu().numpy()\n",
        "    mkpts1 = matched[\"keypoints1\"].cpu().numpy()\n",
        "    descriptors0 = matched[\"descriptors0\"].cpu().numpy()\n",
        "    descriptors1 = matched[\"descriptors1\"].cpu().numpy()\n",
        "    return mkpts0, descriptors0, mkpts1, descriptors1\n",
        "\n",
        "def spatial_entropy(kps, shape, grid_size=4):\n",
        "    H, W = shape[:2]\n",
        "    heatmap = np.zeros((grid_size, grid_size))\n",
        "    for x,y in kps:\n",
        "        col = min(int(x/W*grid_size), grid_size-1)\n",
        "        row = min(int(y/H*grid_size), grid_size-1)\n",
        "        heatmap[row, col] += 1\n",
        "    p = heatmap.flatten(); p /= (p.sum()+1e-8)\n",
        "    return entropy(p)\n",
        "\n",
        "def match_and_metrics(kp1, desc1, kp2, desc2):\n",
        "    d = cdist(desc1, desc2)\n",
        "    idx = np.argmin(d, axis=1)\n",
        "    matched_kp1 = kp1\n",
        "    distances = d[np.arange(len(idx)), idx]\n",
        "    num_matches = len(idx)\n",
        "    efficiency = num_matches / (len(desc1)+1e-8)\n",
        "    avg_distance = float(distances.mean())\n",
        "    spatial_ent = spatial_entropy(matched_kp1, img1.shape)\n",
        "    return idx, distances, num_matches, efficiency, avg_distance, spatial_ent\n",
        "\n",
        "def draw_matches(img1, kp1, img2, kp2, idx, save_path):\n",
        "    h1,w1 = img1.shape[:2]\n",
        "    out = np.zeros((max(img1.shape[0],img2.shape[0]), w1+img2.shape[1], 3),dtype=np.uint8)\n",
        "    out[:h1,:w1]=img1; out[:img2.shape[0],w1:]=img2\n",
        "    for i,j in enumerate(idx):\n",
        "        pt1 = tuple(np.round(kp1[i]).astype(int))\n",
        "        pt2 = tuple(np.round(kp2[j]).astype(int)+np.array([w1,0]))\n",
        "        cv2.line(out,pt1,pt2,(0,255,0),1)\n",
        "    cv2.imwrite(save_path,out)\n"
      ],
      "metadata": {
        "id": "cGLI6DpOF6l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n\n",
        "BASE = '/content/drive/MyDrive/pruebas-pfc-2025/aachen_dataset/day'\n",
        "OUT = '/content/loftr_day_results'\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "results = []\n",
        "\n",
        "# Evaluar\n",
        "for pair in tqdm(sorted(os.listdir(BASE))):\n",
        "    p = os.path.join(BASE, pair)\n",
        "    i1,i2 = os.path.join(p,'img1.png'), os.path.join(p,'img2.png')\n",
        "    if os.path.exists(i1) and os.path.exists(i2):\n",
        "        img1 = cv2.imread(i1); img2 = cv2.imread(i2)\n",
        "        kp1, desc1, kp2, desc2 = extract_loftr(img1, img2)\n",
        "        idx, distances, nm, eff, ad, se = match_and_metrics(kp1, desc1, kp2, desc2)\n",
        "        draw_matches(img1, kp1, img2, kp2, idx, os.path.join(OUT, f\"{pair}_m.png\"))\n",
        "        results.append({'pair': pair, 'num_matches': nm,\n",
        "                         'efficiency': round(eff,4),\n",
        "                         'avg_distance': round(ad,4),\n",
        "                         'spatial_entropy': round(se,4)})\n",
        "\n",
        "# Guardar CSV\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(os.path.join(OUT, 'loftr_day_results.csv'),index=False)"
      ],
      "metadata": {
        "id": "dvCtsHCyGCC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}