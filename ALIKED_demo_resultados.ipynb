{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ALIKED demo resultados"
      ],
      "metadata": {
        "id": "LMPH22Mohfcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "zEGRNRiH0OI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_tD84tvpvjhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPNt1x9mgXSg"
      },
      "outputs": [],
      "source": [
        "# 1. Clonar repositorio\n",
        "!pip install opencv-python kornia torch torchvision --quiet\n",
        "!git clone https://github.com/Shiaoming/ALIKED.git\n",
        "%cd ALIKED"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Instalar dependencias\n",
        "\n",
        "#!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "uz01blz8h35w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Compilar  operaciones personalizadas\n",
        "\n",
        "#%cd /content/ALIKED/custom_ops\n",
        "#!python setup.py build_ext --inplace\n",
        "#!sh build.sh\n",
        "#%cd /content/ALIKED"
      ],
      "metadata": {
        "id": "5U7jsDq1h-MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from custom_ops import get_patches\n",
        "#print(\"Modulo compilado e importado correctamente.\")\n"
      ],
      "metadata": {
        "id": "r8xoXHa9qiLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Importar librerÃ­as y cargar modelo\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import entropy\n",
        "\n",
        "from kornia.feature import ALIKED\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "model = ALIKED(pretrained='outdoor', max_num_keypoints=1000).to(device).eval()  # usamos 1000, pero luego filtramos Top-N(solo prueba)\n"
      ],
      "metadata": {
        "id": "6_GSS_uEvMJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Funciones auxiliares\n",
        "def extract_aliked_features(image, top_n=500):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    img_tensor = torch.from_numpy(gray / 255.).float().unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(img_tensor)\n",
        "\n",
        "    keypoints = out.keypoints[0].cpu().numpy()\n",
        "    scores = out.scores[0].cpu().numpy()\n",
        "    descriptors = out.descriptors[0].cpu().numpy()\n",
        "\n",
        "    # Top-N keypoints por score\n",
        "    if len(scores) > top_n:\n",
        "        idx = np.argsort(scores)[::-1][:top_n]\n",
        "        keypoints = keypoints[idx]\n",
        "        descriptors = descriptors[idx]\n",
        "\n",
        "    return keypoints, descriptors\n",
        "\n",
        "def match_features(desc1, desc2):\n",
        "    distances = cdist(desc1, desc2, metric='euclidean')\n",
        "    indices = np.argmin(distances, axis=1)\n",
        "    min_distances = distances[np.arange(distances.shape[0]), indices]\n",
        "    matches = np.stack((np.arange(len(indices)), indices), axis=1)\n",
        "    return matches, min_distances\n",
        "\n",
        "def spatial_entropy(keypoints, image_shape, grid_size=4):\n",
        "    H, W = image_shape[:2]\n",
        "    heatmap = np.zeros((grid_size, grid_size))\n",
        "    for kp in keypoints:\n",
        "        x, y = kp[:2]\n",
        "        col = min(int(x / W * grid_size), grid_size - 1)\n",
        "        row = min(int(y / H * grid_size), grid_size - 1)\n",
        "        heatmap[row, col] += 1\n",
        "    prob = heatmap.flatten()\n",
        "    prob = prob / (prob.sum() + 1e-8)\n",
        "    return entropy(prob)\n",
        "\n",
        "def draw_matches(img1, kp1, img2, kp2, matches, save_path):\n",
        "    h1, w1 = img1.shape[:2]\n",
        "    h2, w2 = img2.shape[:2]\n",
        "    new_img = np.zeros((max(h1, h2), w1 + w2, 3), dtype=np.uint8)\n",
        "    new_img[:h1, :w1] = img1\n",
        "    new_img[:h2, w1:] = img2\n",
        "    for match in matches:\n",
        "        pt1 = tuple(np.round(kp1[match[0], :2]).astype(int))\n",
        "        pt2 = tuple(np.round(kp2[match[1], :2]).astype(int) + np.array([w1, 0]))\n",
        "        cv2.line(new_img, pt1, pt2, (0, 255, 0), 1)\n",
        "    cv2.imwrite(save_path, new_img)\n"
      ],
      "metadata": {
        "id": "OlX_bN1ivPwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Procesamiento por pares\n",
        "# Ruta base en Google Drive\n",
        "base_path = '/content/drive/MyDrive/pruebas-pfc-2025/aachen_dataset/day'\n",
        "result_dir = '/content/aliked_day_results'\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "csv_data = []\n",
        "\n",
        "for folder in tqdm(sorted(os.listdir(base_path))):\n",
        "    pair_path = os.path.join(base_path, folder)\n",
        "    img1_path = os.path.join(pair_path, 'img1.png')\n",
        "    img2_path = os.path.join(pair_path, 'img2.png')\n",
        "\n",
        "    if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
        "        try:\n",
        "            img1 = cv2.imread(img1_path)\n",
        "            img2 = cv2.imread(img2_path)\n",
        "\n",
        "            kp1, desc1 = extract_aliked_features(img1, top_n=500)\n",
        "            kp2, desc2 = extract_aliked_features(img2, top_n=500)\n",
        "\n",
        "            matches, distances = match_features(desc1, desc2)\n",
        "\n",
        "            num_matches = len(matches)\n",
        "            efficiency = num_matches / (len(desc1) + 1e-8)\n",
        "            avg_distance = float(np.mean(distances))\n",
        "\n",
        "            matched_kp1 = np.array([kp1[m[0]] for m in matches])\n",
        "            spatial_ent = spatial_entropy(matched_kp1, img1.shape)\n",
        "\n",
        "            output_img_path = os.path.join(result_dir, f'{folder}_matches.png')\n",
        "            draw_matches(img1, kp1, img2, kp2, matches, output_img_path)\n",
        "\n",
        "            csv_data.append({\n",
        "                'pair': folder,\n",
        "                'num_matches': num_matches,\n",
        "                'efficiency': round(efficiency, 4),\n",
        "                'avg_distance': round(avg_distance, 4),\n",
        "                'spatial_entropy': round(spatial_ent, 4)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error en {folder}: {e}\")\n",
        "\n",
        "# Guardar CSV\n",
        "df = pd.DataFrame(csv_data)\n",
        "csv_path = os.path.join(result_dir, 'aliked_day_results.csv')\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"Resultados guardados en: {csv_path}\")\n"
      ],
      "metadata": {
        "id": "dVwcmXKqvSXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}